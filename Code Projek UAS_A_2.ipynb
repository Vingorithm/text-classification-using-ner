{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06336b06-2112-47b9-8236-f06295493a89",
   "metadata": {},
   "source": [
    "# **Ujian Akhir Semester - Pengolahan Bahasa Alami**\n",
    "\n",
    "## Anggota Kelompok:\n",
    "- **Kevin Philips Tanamas** (220711789)  \n",
    "- **Richard Angelico** (220711747)\n",
    "- **Anthony Alvin Nathaniel** (220711773)\n",
    "- **Nicholas Raymond Thosimaru** (220712111)\n",
    "- **Maria** (220711969)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200e11b-1806-4bca-abd3-6b004f206437",
   "metadata": {},
   "source": [
    "Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a508b82b-34f9-4db8-a939-dfdf73faf7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 20:14:48.617717: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-23 20:14:49.838740: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750684490.213756  316832 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750684490.297103  316832 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-23 20:14:51.164142: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "import gc\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from seqeval.scheme import BILOU\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Bidirectional, LSTM, GRU, Dense, TimeDistributed, Dropout\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acc946f-13b3-4ad2-a55e-aa26aa1ee59b",
   "metadata": {},
   "source": [
    "Data Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b0d939b-d3ef-4389-b062-8e5788bf1c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force CPU usage for memory-intensive operations\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Use CPU for data preparation\n",
    "\n",
    "# Data loading function\n",
    "def load_data(filename):\n",
    "    sentences, labels = [], []\n",
    "    sentence, label = [], []\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line.strip() == '':\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "            else:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 2:\n",
    "                    word, tag = parts\n",
    "                    sentence.append(word)\n",
    "                    label.append(tag)\n",
    "    return sentences, labels\n",
    "\n",
    "\n",
    "def simple_rnn_model(max_len_seq, emb_size, rnn_units, num_tags, lr=0.001):\n",
    "    \"\"\"\n",
    "    SimpleRNN model for NER - fastest training, but typically lower accuracy\n",
    "\n",
    "    Args:\n",
    "        max_len_seq: Maximum sequence length\n",
    "        emb_size: Embedding size\n",
    "        rnn_units: Number of RNN units\n",
    "        num_tags: Number of output tags\n",
    "        lr: Learning rate\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(max_len_seq, emb_size), dtype='float32')\n",
    "\n",
    "    # Optional projection to reduce dimensions\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "\n",
    "    # SimpleRNN layer - fastest but less capable for sequence modeling\n",
    "    x = tf.keras.layers.SimpleRNN(rnn_units, return_sequences=True)(x)\n",
    "\n",
    "    # Dropout for regularization\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # Output layer with softmax activation for tag prediction\n",
    "    output = TimeDistributed(Dense(num_tags, activation='softmax'))(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74fa4739-f30f-4e75-aca3-a41f36ac0a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilstm_model(max_len_seq, emb_size, rnn_units, num_tags, lr=0.001):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM model for NER - highest accuracy but slowest training\n",
    "\n",
    "    Args:\n",
    "        max_len_seq: Maximum sequence length\n",
    "        emb_size: Embedding size\n",
    "        rnn_units: Number of LSTM units per direction\n",
    "        num_tags: Number of output tags\n",
    "        lr: Learning rate\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(max_len_seq, emb_size), dtype='float32')\n",
    "\n",
    "    # Projection layer to reduce dimensions\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "\n",
    "    # Bidirectional LSTM - processes sequences in both directions\n",
    "    # Higher accuracy for context-dependent tasks like NER\n",
    "    x = Bidirectional(\n",
    "        LSTM(\n",
    "            rnn_units,\n",
    "            return_sequences=True,\n",
    "            recurrent_dropout=0.0,\n",
    "            implementation=2  # potentially faster but less stable\n",
    "        )\n",
    "    )(x)\n",
    "\n",
    "    # Dropout for regularization\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Output layer\n",
    "    output = TimeDistributed(Dense(num_tags, activation='softmax'))(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def bigru_model(max_len_seq, emb_size, rnn_units, num_tags, lr=0.001):\n",
    "    \"\"\"\n",
    "    Bidirectional GRU model - good balance between speed and accuracy\n",
    "\n",
    "    Args:\n",
    "        max_len_seq: Maximum sequence length\n",
    "        emb_size: Embedding size\n",
    "        rnn_units: Number of GRU units per direction\n",
    "        num_tags: Number of output tags\n",
    "        lr: Learning rate\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(max_len_seq, emb_size), dtype='float32')\n",
    "\n",
    "    # Projection layer\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "\n",
    "    # Bidirectional GRU - faster than LSTM with similar capabilities\n",
    "    x = Bidirectional(\n",
    "        GRU(\n",
    "            rnn_units,\n",
    "            return_sequences=True,\n",
    "            recurrent_dropout=0.0,\n",
    "            reset_after=True  # modern GRU implementation\n",
    "        )\n",
    "    )(x)\n",
    "\n",
    "    # Dropout for regularization\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # Output layer\n",
    "    output = TimeDistributed(Dense(num_tags, activation='softmax'))(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e8792-f273-4de0-bdf4-498e802239b0",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8517eb9-026d-4ca1-ba75-e10ce97b6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_file = \"./train-70.txt\"\n",
    "valid_file = \"./dev-30.txt\"\n",
    "test_file = \"./test-data.txt\"\n",
    "\n",
    "train_sentences, train_labels = load_data(train_file)\n",
    "valid_sentences, valid_labels = load_data(valid_file)\n",
    "test_sentences, test_labels = load_data(test_file)\n",
    "\n",
    "# Create dataframes\n",
    "train_df = pd.DataFrame({'tokens': train_sentences, 'tags': train_labels})\n",
    "valid_df = pd.DataFrame({'tokens': valid_sentences, 'tags': valid_labels})\n",
    "test_df = pd.DataFrame({'tokens': test_sentences, 'tags': test_labels})\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\n",
    "    f\"Train samples: {len(train_df)}, Validation samples: {len(valid_df)}, Test samples: {len(test_df)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a69899-1bf6-4180-9a27-219b2bd2c91f",
   "metadata": {},
   "source": [
    "Data Checking (Liat Semua Tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf249be-fdec-46cf-a8da-e01a5fb7ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all tags (flattened) from training labels\n",
    "all_tags_flat = [tag for seq in train_labels for tag in seq]\n",
    "\n",
    "# Print total number of tags\n",
    "print(f\"Total tags in training: {len(all_tags_flat)}\")\n",
    "\n",
    "# Tag distribution in training set\n",
    "print(\"Tag distribution in training set:\")\n",
    "tag_counts = pd.Series(all_tags_flat).value_counts()\n",
    "print(tag_counts.head(10))  # Show top 10 most common tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9877d0-4181-45d8-a7d7-c05891c1128b",
   "metadata": {},
   "source": [
    "Load FastText Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a454dd-b280-4928-ac65-2eab35e29409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Cek dan unduh FastText model jika belum tersedia\n",
    "if not os.path.exists('./cc.id.300.bin'):\n",
    "    print(\"FastText model not found. Downloading model (this may take a while)...\")\n",
    "    os.system(\"wget -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.bin.gz\")\n",
    "    os.system(\"gunzip cc.id.300.bin.gz\")\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "# Load FastText model\n",
    "print(\"Loading FastText model...\")\n",
    "ft = fasttext.load_model('./cc.id.300.bin')\n",
    "print(\"FastText model loaded successfully.\")\n",
    "\n",
    "# Set embedding size dan panjang maksimum urutan\n",
    "emb_size = 300\n",
    "max_seq_len = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9a3f61-cb23-4fad-960e-1daf6553dd58",
   "metadata": {},
   "source": [
    "Word Embeddings Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0468227d-66ec-4b46-8c00-b3108ca3e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat fungsi untuk menghasilkan word embeddings dari token\n",
    "def get_tok_emb(tokens):\n",
    "    emb = np.zeros((max_seq_len, emb_size))\n",
    "    for i, word in enumerate(tokens[:max_seq_len]):\n",
    "        emb[i] = ft.get_word_vector(word)\n",
    "    return emb\n",
    "\n",
    "# Proses pembuatan embeddings untuk setiap token dalam dataset\n",
    "print(\"Creating word embeddings...\")\n",
    "train_df['tokens_embedding'] = train_df['tokens'].progress_apply(get_tok_emb)\n",
    "valid_df['tokens_embedding'] = valid_df['tokens'].progress_apply(get_tok_emb)\n",
    "test_df['tokens_embedding'] = test_df['tokens'].progress_apply(get_tok_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6dee39-65db-4304-a700-9d94516b4318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Untuk Pengolahan Bahasa Alami",
   "language": "python",
   "name": "kernel_bahasa_alami"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
